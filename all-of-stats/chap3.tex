\documentclass{article}
\include{common}
\begin{document}
\begin{prop}\label{c3p1}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X: \Omega \rightarrow
\sor$ be a random variable. Then $\mu_F:\mathcal{F} \rightarrow [0, 1]$, where
$\mu_F((a, b)) = F(b) - F(a)$ is a probability measure on the measurable space 
$(\sor, \mathcal{B})$.
\end{prop}
\begin{proof}
We first note that $\mu_F(\sor) = 1$. Let $I_1 = (a, b)$ and $I_2 = (c, d)$ be
disjoint intervals and $I= I_1 \cup I_2$. Now, the probability of $X$ being in
$I$ is just the sum of probabilities in $I_1$ and $I_2$. Therefore $\mu_F(I) =
P(X \in I) = P(X \in I_1) + P(X \in I_2) = F(b) - F(a) + F(d) - F(c) =\mu_F(I_1)
+ \mu_F(I_2)$. Thus, $\mu_F$ is indeed a probability measure.
\end{proof}
\begin{rem}
Since $(\sor, \mathcal{B}, \mu_F)$ is a probability space, we can define 
integrals like $\int_A g(x)d\mu_F$ as Lebesgue integrals. It is common to 
express this integral as $\int_A gdF$.
\end{rem}

One can also interpret an integral $\int gdF$ as $\int g(x)f(x)dx$ as $f=F^\op$,
that is a Riemann-Stieltje integral. However, this depends on $F$ being 
differentiable. Instead of getting mired in the measure theoretic issues, we 
will settle with the Riemann-Steiltje interpretation and move on.

\begin{defn}\label{c3d1}
The expected value of a random variable is defined to be
\[
E(X) = \int xdF = \begin{cases}
\sum_x xf(x) \text{ if } X \text{ is discrete} \\
\int xf(x)dx \text{ if } X \text{ is continuous.}
\end{cases}
\]
$E(X)$ is also denoted as $\mu_X$.
\end{defn}

\begin{rem}
If $X$ is discrete then $F$ is a (normalised) counting measure. It is the 
probability of $P(X=x)$, which is the ratio of count of the events in $\Omega$
for which $X = x$ divided by the count of all events. If $p_k$ denotes $P(X=k)$
then $\mu_X = \sum_k kp_k$.
\end{rem}

\begin{prop}\label{c3p2}
Let $X$ be a discrete random variable and $Y = r(X)$, where $r$ is a bijective
map. Then 
\[
E(Y) = \int r(X)dF_X = \sum_x r(x)f_X(x).
\]
\end{prop}
\begin{proof}
$E(Y) = \sum_y yf_Y(y)$. Since $r$ is bijective, the sum over $y$ can be 
replaced with that over $x$ so that $E(Y) = \sum_x r(x)f_Y(y)$. Now $f_Y(y)
= F_Y(y) - F_Y(y - 1) = P(Y \le y) - P(Y \le y - 1) = P(r(X) \le y) - P(r(X) \le
y - 1) = P(X \le r^{-1}(y)) - P(X \le r^{-1}(y-1)) = F_X(r^{-1}(y)) - F_X(r^{-1}
(y - 1)) = f_X(x)$. Thus, we have
\[
E(Y) = \sum_x r(x)f_X(x).
\]
\end{proof}

\begin{thm}[The Rule of the Lazy Statistician]\label{c3t1}
Let $X$ be a random variable and $Y = r(X)$, then 
\[
\mu_Y = E(r(X)) = \int rdF_X.
\]
\end{thm}
\begin{proof}
\[
E(r(X)) = \int r(x)f_X(x)dx = \int r(x)dF_X.
\]
\end{proof}

If $r(x) = I_A(x)$, the indicator function of the set $A$ then,
\[
E(r(X)) = \int I_A(x)dF_x = \int_A df_X(x) = P(X \in A).
\]

\section{Solutions to problems}
\begin{enumerate}
\item Solved in the accompanying Python notebook.

\item If $P(X = c) = 1$ then $E(X) = c$ and $E(X^2) = c^2$ so that $\var(X) = 0$.
If, on the other hand, $\var(X) = 0$ then $E[(X - E(X))^2] = 0$. Now
$(X - E(X))^2$ is a non-negative random variable. If its expectation is zero then
it should be zero, almost surely. That is $X = E(X)$ almost surely, which in turn
means that $X$ is a constant.

\item Solved in the accompanying Python notebook.

\item Solved in the accompanying Python notebook.

\item Let $Y$ be the random variable to count the number of tosses needed before
a coin lands head up. If $p$ is the probability of landind head then 
\[
P(Y = n) = p^{n-1}q.
\]
Therefore,
\[
E(Y) = \sum_{n \ge 1}nP(Y = n) = \sum_{n \ge 1}np^{n-1}q = q\sum_{n \ge 1}np^{n-1}.
\]
Now,
\[
\sum_{n \ge 1}p^n = \frac{1}{1-p} \Rightarrow \sum_{n \ge 1}np^{n-1} = \frac{1}{(1 - p)^2},
\]
so that
\[
E(Y) = \frac{q}{(1 - p)^2} = \frac{1}{1 - p}.
\]

\item Let $Y = r(X)$. When $X$ takes values $x_1, x_2, \ldots$ with probability
$p_1, p_2, \ldots$ then $Y$ takes values $r(x_1), r(x_2), \ldots$ with the same 
probability. Therefore, its mean value is
\[
E(Y) = \sum_{k\ge 1}r(x_k)p_k = \int r(x)dF.
\]

\item Let us assume that
\[
\lim_{x \rightarrow \infty} x(1 - F(x)) = 0.
\]
Then,
\begin{eqnarray*}
E(X) &=& \int_0^\infty xdF \\
 &=& \lim_{x^\op\rightarrow\infty}\int_0^{x^\op}xdF \\
 &=& \lim_{x^\op\rightarrow\infty}\left(x^\op F(x^\op)-\int_0^{x^\op}F(x)dx\right)\\
 &=& \lim_{x^\op\rightarrow\infty}\left(x^\op F(x^\op) - \int_0^{x^\op}P(X \le x) dx \right) \\
 &=& \lim_{x^\op\rightarrow\infty}\left(x^\op F(x^\op) - \int_0^{x^\op}(1 - P(X > x)) dx \right) \\
 &=& \lim_{x^\op\rightarrow\infty}(x^\op F(x^\op) - x^\op) + \int_0^\infty P(X > x)dx \\
 &=&  \int_0^\infty P(X > x)dx
\end{eqnarray*}

\item Since,
\[
\bar{X}_n = \frac{1}{n}\sum_{i = 1}^n X_i,
\]
we have
\[
E(\bar{X}_n) = \frac{1}{n}\sum{i=1}^nE(X_i) = \frac{1}{n}\sum_{i=1}^n\mu = \mu.
\]
We will now compute
\begin{equation}\label{e1}
\var(\bar{X}_n) = E(\bar{X}_n^2) - E^2(\bar{X}_n) = E(\bar{X}_n^2) - \mu^2
\end{equation}
Now,
\[
E(\bar{X}_n^2) = E\left(\frac{1}{n}\sum_{i=1}^nX_i\right)^2
\]
so that
\[
n^2E(\bar{X}_n^2) = \sum_{i=1}^n E(X_i^2) + 2\sum_{i=1}^n\sum_{j=i+1}^n E(X_iX_j).
\]
Since $X_i$ are all independent, $E(X_iX_j) = E(X_i)E(X_j) = \mu^2$. Furthermore,
$E(X_i^2) = \mu^2 + \sigma^2$ so that,
\[
n^2E(\bar{X}_n^2) = n(\mu^2 + \sigma^2) + 2\frac{n(n-1)}{2}\mu^2 \Rightarrow
E(\bar{X}_n^2) = \frac{\sigma^2}{n} + \mu^2,
\]
so that from equation \eqref{e1} we get
\[
\var(\bar{X}_n) = \frac{\sigma^2}{n}.
\]

Now consider, $(n-1)S_n^2 = \sum_{i=1}^n(X_i - \bar{X}_n)^2$ so that
\begin{equation}\label{e2}
(n-1)E(S_n^2) = \sum_{i=1}^n\left(E(X_i^2) - 2E(X_i\bar{X}_n) + E(\bar{X}_n^2)\right)
\end{equation}
We first consider the term $E(X_i\bar{X}_n)$. It is
\begin{eqnarray*}
E(X_i\bar{X}_n) &=& E\left(X_i \sum_{j=1}^n\frac{X_j}{n}\right) \\
nE(X_i\bar{X}_n) &=& \sum_{j=1, j\ne i}^n E(X_iX_j) + E(X_i^2) \\
nE(X_i\bar{X}_n) &=& (n-1)\mu^2 + \mu^2 + \sigma^2 \\
E(X_i\bar{X}_n) &=& \mu^2 + \frac{\sigma^2}{n},
\end{eqnarray*}
as $X_i$ are iid. We have evaluated the other terms in \eqref{e2} previously.
Thus,
\[
(n-1)E(S_n^2) =\sum_{i=1}^n
\left(\mu^2+\sigma^2-2\mu^2-2\frac{\sigma^2}{n}+\frac{\sigma^2}{n}+\mu^2\right)
= \sum_{i=1}^n\left(\sigma^2 - \frac{\sigma^2}{n}\right)
\]
so that $E(S_n^2) = \sigma^2$.

\item Solved in the accompanying Python notebook.

\item Given that 
\[
f_X = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}
\]
and $Y = \exp(X)$. Therefore,
\[
E(Y) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-x^2/2 + x} dx.
\]
Now,
\[
-x^2/2 + x = \frac{1}{2}(-x^2 + 2x - 1 + 1) = -\frac{(x-1)^2}{2} + \frac{1}{2}
\]
so that
\[
E(Y) = \frac{e^{1/2}}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-(x - 1)^2/2}dx.
\]
A substitution $u = x - 1$ leads to $E(Y) = e^{1/2}$.

We next compute $E(Y^2) = E(e^{2X})$, which is
\[
E(Y^2) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-x^2/2 + 2x} dx.
\]
Now,
\[
-\frac{x^2}{2} + 2x = -\frac{1}{2}(x^2 - 4x + 4) + 2
\] 
so that $E(Y^2) = e^2$ and hence $\var(Y) = e^2 - e$.

\item Solved in the accompanying Python notebook.

\item To do.

\item Let $B$ be a Bernoulli random variable, $U_1 \sim \dUni(0, 1)$ and $U_2 
\sim \dUni(3, 4)$. Then
\[
Y = BU_1 + (1 - B)U_2
\]
so that $E(Y) = E(BU_1) + E((1 - B)U_2)$. $B$ and $U_1$ are independent. So are
$1 - B$ and $U_2$. Therefore, 
\[
E(Y) = \frac{p}{1} + (1 - p)\frac{7}{2} = \frac{p + 7q}{2}.
\]
To compute the variance, we need $E(Y^2)$. $Y^2 = B^2U_1^2 + (1 - B)^2U_2^2 +
B(1 - B)U_1U_2$. If $X = \dUni(a, b)$, 
\[
E(X^2) = \int_a^b x^2dx = \frac{b^3 - a^3}{3}
\]
so that $E(U_1^2) = 1/3$ and $E(U_2^2) = 37/3$. $E(B_1^2) = p, E((1 - B)^2) = q$.
Therefore,
\[
E(Y^2) = \frac{p}{3} + \frac{37q}{3}
\]
and hence,
\[
\var(Y) = \frac{p}{3} + \frac{37q}{3} - \frac{(p + 7q)^2}{4}.
\]
If $p = q = 1/2$ then
\[
\var(Y) = \frac{7}{3}.
\]

\item $\cov(X, Y) = E(XY) - E(X)E(Y)$ so that
\begin{eqnarray*}
\cov\left(\sum_{i=1}^ma_iX_i, \sum_{i=1}^nb_iY_i\right) &=& 
E\left(\sum_{i=1}^m\sum_{j=1}^na_ib_jX_iY_j\right) - 
E\left(\sum_{i=1}^ma_iX_i\right)E\left(\sum_{i=1}^nb_iY_i\right) \\
 &=& \sum_{i=1}^m\sum_{j=1}^na_ib_jE(X_iY_j) - 
 \sum_{i=1}^ma_iE(X_i)\sum_{j=1}^nb_jE(Y_j) \\
 &=& \sum_{i=1}^m\sum_{j=1}^na_ib_j\cov(X_i, Y_j)
\end{eqnarray*}

\item We will first get a formula for the variance of a linear combination of
random variables that are not necessarily independent. Let $Y = \sum_{i=1}^na_i
X_i$. Then $E(Y) = \sum_{i=1}^na_iE(X_i)$. However,
\[
Y^2 = \sum_{i=1}^n\sum_{j=1}^n a_ia_jX_iX_j = \sum_{i=1}^na_i^2X_i^2 + 
\sum_{i=1}^n\sum{j=i+1}^na_ia_j X_iX_j
\]
so that
\begin{equation}\label{e3}
E(Y^2) = \sum_{i=1}^na_i^2E(X_i^2) + 2\sum_{i=1}^n\sum_{j=i+1}^na_ia_jE(X_iX_j)
\end{equation}
and 
\begin{equation}\label{e4}
E^2(Y) = \sum_{i=1}^n a_i^2E^2(X_i) + 
2\sum_{i=1}^n\sum_{j=i+1}^n a_ia_jE(X_i)E(X_j).
\end{equation}
Therefore,
\begin{equation}\label{e5}
\var(Y) = \sum_{i=1}^na_i^2\var(X_i) + 2\sum_{i=1}^n\sum_{j=i+1}^n\cov(X_i,X_j).
\end{equation}

If $Z = 2X - 3Y + 8)$, 
\begin{equation}\label{e6}
\var{Z} = 4\var{X} + 9\var{Y} + 0 - 12\cov(X, Y)+0+0.
\end{equation}
Now,
\[
\var{X} = \iint f(x, y)x^2dxdy - \left(\iint f(x, y)xdxdy\right)^2
\]
where the range of integration is over $[0, 1] \times [0, 2]$ and $f(x, y) = 
(x + y)/3$.
\begin{eqnarray*}
\iint f(x, y)x^2dxdy &=& \frac{1}{3}\iint(x^3 + x^2y)dxdy \\
 &=& \frac{1}{3}\int_0^1x^3dx\int_0^2 dy + \frac{1}{3}\int_0^1x^2dx\int_0^2ydy \\
 &=& \frac{1}{3}\frac{1}{4}2 + \frac{1}{3}\frac{1}{3}2 \\
 &=& \frac{7}{18}.
\end{eqnarray*}
\begin{eqnarray*}
\iint f(x, y)xdxdy &=& \frac{1}{3}\int_0^1x^2dx\int_0^2dy + 
 \frac{1}{3}\int_0^1xdx\int_0^2ydy \\
 &=& \frac{1}{3}\frac{1}{3}2 + \frac{1}{3}\frac{1}{2}2 \\
 &=& \frac{5}{9}
\end{eqnarray*}
so that 
\begin{equation}\label{e7}
\var(X) = 7/18 - 25/81 = 13/162.
\end{equation}
Similarly,
\[
\var{Y} = \iint f(x, y)y^2dxdy - \left(\iint f(x, y)ydxdy\right)^2
\]
\begin{eqnarray*}
\iint f(x, y)y^2dxdy &=& \frac{1}{3}\iint xy^2dxdy + \frac{1}{3}\iint y^3dxdy \\
 &=& \frac{1}{3}\int_0^1xdx\int_0^2y^2dy + \frac{1}{3}\int_0^1dx\int_0^2y^3dy \\
 &=& \frac{1}{3}\frac{1}{2}\frac{8}{3} + \frac{1}{3}\;1\;4 \\
 &=& \frac{16}{9}
\end{eqnarray*}
\begin{eqnarray*}
\iint f(x, y)ydxdy &=& \frac{1}{3}\iint xydxdy + \frac{1}{3}\iint y^2dxdy \\
 &=& \frac{1}{3}\int_0^1xdx\int_0^2ydy + \frac{1}{3}\int_0^1dx\int_0^2y^2dy \\
 &=& \frac{1}{3}\frac{1}{2}2 + \frac{1}{3}\;1\;\frac{8}{3} \\
 &=& \frac{11}{9}
\end{eqnarray*}
so that 
\begin{equation}\label{e8}
\var Y = 16/9 - 121/81 = 23/81.
\end{equation}
Likewise,
\[
\cov(XY) = E(XY) = E(X)E(Y)
\]
so that
\begin{eqnarray}
\cov(X,Y) &=& \frac{1}{3}\iint (x^2y + xy^2)dxdy - \frac{1}{3}\iint(x^2+xy)dxdy
\frac{1}{3}\iint(xy + xy^2)dxdy \nonumber* \\
 &=& \frac{1}{3}\frac{2}{3}\frac{4}{3} - \frac{1}{3}\frac{2}{3}1
 \frac{1}{3}\frac{4}{3}1 \nonumber* \\
 &=& \frac{16}{81} \label{e9}
\end{eqnarray}
Finally, from equations \eqref{e5} to \eqref{e9}
\[
\var Z = \frac{26}{81} + \frac{23}{9} - \frac{192}{81} = \frac{41}{81}.
\]
\end{enumerate}
 
\end{document}