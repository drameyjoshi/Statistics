\documentclass{article}
\include{common}
\begin{document}
\begin{defn}\label{c2d1}
Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$.
Then members of $\mathcal{F}$ are called measurable sets.
\end{defn}

If a set $A$ is in $\sigma$-algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ then it
is also in the $\sigma$-algebra $\mathcal{F}_1 \cap \mathcal{F}_2$. The smallest
$\sigma$-algebra containing $A$ is called the $\sigma$-algebra generated by $A$
and is denoted by $\sigma(A)$. An example of such a $\sigma$-algebra is the one
generated by open intervals of $\sor$. It is called the Borel $\sigma$-algebra.

\begin{defn}\label{c2d2}
Let $(\Omega, \mathcal{F})$ be a measurable space. A function $m:\Omega\rightarrow
[0, \infty]$ is called a measure if
\begin{itemize}
\item $m(\varnothing) = 0$,
\item If $A_1, A_2, \ldots$ are pair-wise disjoint members of $\mathcal{F}$ then
\[
m\left(\bigcup_{k \ge 1}A_k\right) = \sum_{k \ge 1}m(A_k).
\]
\end{itemize}
\end{defn}
If the range of $m$ is $[0, 1]$ then it is called the probability measure and is
denoted by $P$. The triple $(\Omega, \mathcal{F}, m)$ is called a measure space
while $(\Omega, \mathcal{F}, m)$ is called a probability space. For a probability
space, $P(\Omega) = 1$. Since  $A \cup A^c = \Omega$ is a disjoing union of 
members of $\mathcal{F}$, it follows that $P(A) + P(A^c) = P(\Omega) = 1$.

\begin{defn}\label{c2d3}
Let $(X, \Sigma_X)$ and $(Y, \Sigma_Y)$ be measurable spaces. A map $f: X 
\rightarrow Y$ is said to be measurable if $f^{-1}(B) \in \Sigma_X$ for all
$B \in \Sigma_Y$.
\end{defn}

\begin{defn}\label{c2d4}
Let $(\Omega, \mathcal{F}, P)$ be a probability space then a random variable
is a measurable map $X: \Omega \rightarrow \sor$, where it is understood that
the $\sigma$-algebra on $\sor$ is the Borel $\sigma$-algebra. In other words,
if for every open interval $(a, b)$, $P^{-1}((a, b)) \in \mathcal{F}$.
\end{defn}

Examples of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ be the sample space of a coin-toss. Then $X(H)=1,
X(T)=0$ is a random variable.
\item If $\Omega = \{H, T\}$ and a person wins $20$ quid if the coin lands head
or loses $10$ if the coin lands tail then $X(H) = 20, X(T) = -10$ is also a 
random variable. Thus, a random variable associates a real number to every 
outcome of an experiment.
\item If $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ then $X(\omega) = x, Y(
\omega) = y, R(\omega) = \sqrt{x^2 + y^2}, L(\omega) = |x + y|$ are some examples
of random variables.
\end{enumerate}

\begin{defn}\label{c2d5}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X$ be a random variable
on it. If $A$ is a measurable subset of $\sor$ then $P(A) = P(X^{-1}(A))$.
\end{defn}

Thus, $P$ is also a probability measure on $(\sor, \mathcal{B}, m)$ where 
$\mathcal{B}$ is the Borel $\sigma$-algebra on $\sor$ and $m$ is a Lebesgue 
measure. (That is, if $I = (a, b)$ then $m(I) = b - a$.)

Examples of probability of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ such that $P(H) = 0.3, P(T) = 0.7$. If $X(H)=1,
X(T)=0$ then $P(1) = 0.3, P(0) = 0.7$.
\item If $\Omega = \{HH, HT, TH, TT\}$ be the sample space of two tosses of a
coin for which the probability of getting a head is $p$. If $P(HH) = 2, P(HT) = 
P(TH) = 1, P(TT) = 0$ then $P(2) = p^2, P(1) = 2p(1-p),P(0) = (1 - p)^2$.
\item Let $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ denote a dart-board whose
payoff is denoted by 
\[
X(\omega) = \begin{cases}
0 \text{ if } \sqrt{x^2 + y^2} > 1/2 \\
1 \text{ if } \sqrt{x^2 + y^2} \le 1/2,
\end{cases}
\]
where $\omega$ is the point where the dart lands and its coordinates are 
$(x, y)$, then $P(X = 0) = 1/2$ and $P(X = 1) = 1/2$.
\end{enumerate}

\begin{defn}\label{c2d6}
Let $X$ be a random variable on a probability space $(\Omega, \mathcal{F}, P)$.
Its cumulative distribution function $F_X: \sor \rightarrow [0, 1]$ is defined
as $F_X(x) = P(X \le x)$.
\end{defn}

Unless otherwise stated, we will always assume that all random variables are 
defined over a probability space $(\Omega, \mathcal{F}, P)$.
\begin{prop}\label{c2p1}
Let $X$ be a random variable with cfd $F$. If $I$ is the interval $(a, b)$ then
$P(I) = F(b) - F(a)$.
\end{prop}
\begin{proof}
$F(b) = P(X \le b) = P(\omega \in X^{-1}((-\infty, b]))$. Since $(-\infty, a]
\cup (a, b] = (-\infty, b]$ and the union is of disjoint sets,
\[
P((-\infty, A]) + P((a, b]) = P((-\infty, b]) \Rightarrow F(a) + P((a, b]) 
= F(b).
\]
Now $P(I) = P((a, b)) = P((a, b])$ because $(a, b] = (a, b) \cup \{b\}$ and 
$P(\{b\}) = 0$. Therefore, $P(I) = F(b) - F(a)$.
\end{proof}

\begin{thm}\label{c2t1}
If $X$ and $Y$ are random variables with cdfs $F$ and $G$ then $F(x) = G(x)
\Rightarrow P(X \in A) = P(Y \in A)$ for all measurable sets $A$. 
\end{thm}
\begin{proof}
Here $A$ is a measurable subset of $\mathcal{B}$, the Borel $\sigma$-algebra on
$\sor$. It can be written as a countable union of open intervals or their 
complements. Any countable union of sets can be written as a countable union of
disjoint sets. Therefore, any $A \in \mathcal{B}$ can be written as a countable
union of pair-wise disjoint open intervals.

Since $F(x) = G(x)$, by proposition \ref{c2p1}, $P(X \in A) = P(Y \in A)$ for all
open intervals $A$. Since any measurable set can be written as a countable union
of open intervals or their complements and for each of these sets, $A$, 
$P(X \in A) = P(Y \in A)$, the equality is also true for all measurable sets.
\end{proof}

\begin{thm}\label{c2t2}
Let $F$ be the cdf of a random variable $X$ then:
\begin{enumerate}
\item $x_1 < x_2 \Rightarrow F(x_1) \le F(x_2)$,
\item $F(x) \rightarrow 0$ as $x \rightarrow -\infty$ and $F(x) \rightarrow 1$
as $x \rightarrow \infty$ and
\item $F$ is right continuous for all $x \in \sor$.
\end{enumerate}
\end{thm}
\begin{proof}
$F(x_2) = P(X \le x_2) = P(\omega \in (-\infty, x_2]) = P(\omega \in (-\infty, x_1]
\cup (x_1, x_2]) = P(X \le x_1) + P((x_1, x_2]) \ge F(x_1)$.

As $x \rightarrow \infty$, the set $X \le x$ becomes all of $\sor$ so that 
$X^{-1}(\sor)$ is $\Omega$ and hence $P(X^{-1}(\sor)) = P(\Omega) = 1$. Likewise,
as $x \rightarrow -\infty$, the set $X \le x$ becomes $\varnothing$ for which
the probability is zero.

To prove right continuity at $x$, consider a monotonically decreasing sequence 
$\{y_n\}$ such that $y_n \rightarrow x$. If $A_n = (-\infty, y_n]$ then $A_1
\supseteq A_2 \supseteq \ldots$ and $\cap_{n \ge 1}A_n = A$; $A$ is a subset of
all $A_n$. Therefore,
\[
\lim_{n \rightarrow \infty}P(A_n) = P(A)
\]
that is, 
\[
\lim_{y_n \rightarrow x^+}F(y_n) = F(x),
\]
so that $F$ is right continuous.
\end{proof}
\begin{rem}
The same reasoning does not work from the left side. Suppose the sequence $\{y_n\}$
is monotonically increasing with limit $x$ and we consider the sets $A_n = (-\infty,
y_n]$. Then the union of these sets is $(-\infty, x)$ and not $(-\infty, x]$.
\end{rem}

\begin{defn}\label{c2d7}
A random variable whose range is a countable set is called a discrete random 
variable. If $X$ is a discrete random variable, its probability mass function,
also called pmf, is $f_X(x) = P(X = x)$.
\end{defn}

In terms of $f$, $F(x) = \sum_{x^\op \le x}P(x^\op)$.

\begin{defn}\label{c2d8}
A random variable whose range is uncountable is called a continuous random
variable. If $X$ is a continuous random variable, its density function $f$ is
defined as
\[
\int_a^b f(x)dx = P(a \le X \le b),
\]
where $F$ is its cdf.
\end{defn}

In terms of $f$,
\[
F(x) = \int_{-\infty}^x f(x)dx.
\]

\begin{prop}\label{c2p2}
Let $F$ be the cdf of a random variable $X$. Then:
\begin{enumerate}
\item $P(X=x) = F(x) - F(x^-)$, where $F(x^-) = \lim_{y \uparrow x}F(y)$.
\item $P(x < X \le y) = F(y) - F(x)$.
\item $P(X > x) = 1 - F(x)$.
\item If $X$ is continuous then $F(b) - F(a) = P(a < x < b) = P(a \le x < b) =
P(a < x \le b) = P(a \le x \le b)$.
\end{enumerate}
\end{prop}
\begin{proof}
Let $\{y_n\}$ be a monotone increasing sequence of numbers with limit $x$. Then
$P(X \le y_n) = P(A_n)$, where $A_n = (-\infty, y_n]$. The sets $\{A_n\}$ are 
also monotone increasing with union $A = (-\infty, x)$. Therefore,
\[
\lim_{n \rightarrow \infty}P(X \le y_n) = P(A).
\]
Since $(-\infty, x] = (-\infty, x) + \{x\}$, $P(X=x) = F(x) - F(x^-)$.

The set $A = \{x < X \le y\}$ can be written as $(-\infty, x] \cup A = 
(-\infty, y]$. Therefore, $P(A) = F(y) - F(x)$.

Since the set $X > x$ is a complement of the set $X \ge x$ or equivalently,
$x \le X$, $P(X > x) = 1 - P(x \le X) = 1 - F(x)$.

If $F$ is continuous then $F(x^-) = F(x) = F(x^+)$ so that $P(X=x) = 0$. Since
$(a, b) \cup {b} = (a, b]$, $P(X \in (a, b)) + P(X = b) = P(X \in (a, b])$ so 
that $P(X \in (a, b)) = P(X \in (a, b])$. Other relations can be similarly 
proved.
\end{proof}

\begin{defn}\label{c2d8}
Let $X$ be a random variable with cdf $F$. Then its quantile function $Q$ is 
defined as $Q(p) = \inf\{x \;|\; F(x) \ge p\}$, for $p \in [0, 1]$.
\end{defn}
\begin{rem}
The right continuity of $F$ makes $\inf\{x \;|\; F(x) \ge p\} = 
\inf\{x \;|\; F(x) > p\}$.
\end{rem}

\begin{defn}\label{c2d9}
Random variables $X$ and $Y$ are said to be equal in distribution if $F_X(x) = 
F_Y(x)$ for all $x \in \sor$. Equality in distribution is written as $X
\stackrel{d}{=} Y$.
\end{defn}
\end{document}