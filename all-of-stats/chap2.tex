\documentclass{article}
\include{common}
\begin{document}
\begin{defn}\label{c2d1}
Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra on 
$\Omega$.  Then members of $\mathcal{F}$ are called measurable sets.
\end{defn}

If a set $A$ is in $\sigma$-algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ then it
is also in the $\sigma$-algebra $\mathcal{F}_1 \cap \mathcal{F}_2$. The smallest
$\sigma$-algebra containing $A$ is called the $\sigma$-algebra generated by $A$
and is denoted by $\sigma(A)$. An example of such a $\sigma$-algebra is the one
generated by open intervals of $\sor$. It is called the Borel $\sigma$-algebra.

\begin{defn}\label{c2d2}
Let $(\Omega, \mathcal{F})$ be a measurable space. A function $m:\Omega
\rightarrow [0, \infty]$ is called a measure if
\begin{itemize}
\item $m(\varnothing) = 0$,
\item If $A_1, A_2, \ldots$ are pair-wise disjoint members of $\mathcal{F}$ then
\[
m\left(\bigcup_{k \ge 1}A_k\right) = \sum_{k \ge 1}m(A_k).
\]
\end{itemize}
\end{defn}
If the range of $m$ is $[0, 1]$ then it is called the probability measure and is
denoted by $P$. The triple $(\Omega, \mathcal{F}, m)$ is called a measure space
while $(\Omega, \mathcal{F}, m)$ is called a probability space. For a probability
space, $P(\Omega) = 1$. Since  $A \cup A^c = \Omega$ is a disjoing union of 
members of $\mathcal{F}$, it follows that $P(A) + P(A^c) = P(\Omega) = 1$.

\begin{defn}\label{c2d3}
Let $(X, \Sigma_X)$ and $(Y, \Sigma_Y)$ be measurable spaces. A map $f: X 
\rightarrow Y$ is said to be measurable if $f^{-1}(B) \in \Sigma_X$ for all
$B \in \Sigma_Y$.
\end{defn}

\begin{defn}\label{c2d4}
Let $(\Omega, \mathcal{F}, P)$ be a probability space then a random variable
is a measurable map $X: \Omega \rightarrow \sor$, where it is understood that
the $\sigma$-algebra on $\sor$ is the Borel $\sigma$-algebra. In other words,
if for every open interval $(a, b)$, $P^{-1}((a, b)) \in \mathcal{F}$.
\end{defn}

Examples of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ be the sample space of a coin-toss. Then $X(H)=1,
X(T)=0$ is a random variable.
\item If $\Omega = \{H, T\}$ and a person wins $20$ quid if the coin lands head
or loses $10$ if the coin lands tail then $X(H) = 20, X(T) = -10$ is also a 
random variable. Thus, a random variable associates a real number to every 
outcome of an experiment.
\item If $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ then $X(\omega) = x, Y(
\omega) = y, R(\omega) = \sqrt{x^2 + y^2}, L(\omega) = |x + y|$ are some examples
of random variables.
\end{enumerate}

\begin{defn}\label{c2d5}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X$ be a random variable
on it. If $A$ is a measurable subset of $\sor$ then $P(A) = P(X^{-1}(A))$.
\end{defn}

Thus, $P$ is also a probability measure on $(\sor, \mathcal{B}, m)$ where 
$\mathcal{B}$ is the Borel $\sigma$-algebra on $\sor$ and $m$ is a Lebesgue 
measure. (That is, if $I = (a, b)$ then $m(I) = b - a$.)

Examples of probability of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ such that $P(H) = 0.3, P(T) = 0.7$. If $X(H)=1,
X(T)=0$ then $P(1) = 0.3, P(0) = 0.7$.
\item If $\Omega = \{HH, HT, TH, TT\}$ be the sample space of two tosses of a
coin for which the probability of getting a head is $p$. If $P(HH) = 2, P(HT) = 
P(TH) = 1, P(TT) = 0$ then $P(2) = p^2, P(1) = 2p(1-p),P(0) = (1 - p)^2$.
\item Let $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ denote a dart-board whose
payoff is denoted by 
\[
X(\omega) = \begin{cases}
0 \text{ if } \sqrt{x^2 + y^2} > 1/2 \\
1 \text{ if } \sqrt{x^2 + y^2} \le 1/2,
\end{cases}
\]
where $\omega$ is the point where the dart lands and its coordinates are 
$(x, y)$, then $P(X = 0) = 1/2$ and $P(X = 1) = 1/2$.
\end{enumerate}

\begin{defn}\label{c2d6}
Let $X$ be a random variable on a probability space $(\Omega, \mathcal{F}, P)$.
Its cumulative distribution function $F_X: \sor \rightarrow [0, 1]$ is defined
as $F_X(x) = P(X \le x)$.
\end{defn}

Unless otherwise stated, we will always assume that all random variables are 
defined over a probability space $(\Omega, \mathcal{F}, P)$.
\begin{prop}\label{c2p1}
Let $X$ be a random variable with cfd $F$. If $I$ is the interval $(a, b)$ then
$P(I) = F(b) - F(a)$.
\end{prop}
\begin{proof}
$F(b) = P(X \le b) = P(\omega \in X^{-1}((-\infty, b]))$. Since $(-\infty, a]
\cup (a, b] = (-\infty, b]$ and the union is of disjoint sets,
\[
P((-\infty, A]) + P((a, b]) = P((-\infty, b]) \Rightarrow F(a) + P((a, b]) 
= F(b).
\]
Now $P(I) = P((a, b)) = P((a, b])$ because $(a, b] = (a, b) \cup \{b\}$ and 
$P(\{b\}) = 0$. Therefore, $P(I) = F(b) - F(a)$.
\end{proof}

\begin{thm}\label{c2t1}
If $X$ and $Y$ are random variables with cdfs $F$ and $G$ then $F(x) = G(x)
\Rightarrow P(X \in A) = P(Y \in A)$ for all measurable sets $A$. 
\end{thm}
\begin{proof}
Here $A$ is a measurable subset of $\mathcal{B}$, the Borel $\sigma$-algebra on
$\sor$. It can be written as a countable union of open intervals or their 
complements. Any countable union of sets can be written as a countable union of
disjoint sets. Therefore, any $A \in \mathcal{B}$ can be written as a countable
union of pair-wise disjoint open intervals.

Since $F(x) = G(x)$, by proposition \ref{c2p1}, $P(X \in A) = P(Y \in A)$ for all
open intervals $A$. Since any measurable set can be written as a countable union
of open intervals or their complements and for each of these sets, $A$, 
$P(X \in A) = P(Y \in A)$, the equality is also true for all measurable sets.
\end{proof}

\begin{thm}\label{c2t2}
Let $F$ be the cdf of a random variable $X$ then:
\begin{enumerate}
\item $x_1 < x_2 \Rightarrow F(x_1) \le F(x_2)$,
\item $F(x) \rightarrow 0$ as $x \rightarrow -\infty$ and $F(x) \rightarrow 1$
as $x \rightarrow \infty$ and
\item $F$ is right continuous for all $x \in \sor$.
\end{enumerate}
\end{thm}
\begin{proof}
$F(x_2) = P(X \le x_2) = P(\omega \in (-\infty, x_2]) = P(\omega \in (-\infty, x_1]
\cup (x_1, x_2]) = P(X \le x_1) + P((x_1, x_2]) \ge F(x_1)$.

As $x \rightarrow \infty$, the set $X \le x$ becomes all of $\sor$ so that 
$X^{-1}(\sor)$ is $\Omega$ and hence $P(X^{-1}(\sor)) = P(\Omega) = 1$. Likewise,
as $x \rightarrow -\infty$, the set $X \le x$ becomes $\varnothing$ for which
the probability is zero.

To prove right continuity at $x$, consider a monotonically decreasing sequence 
$\{y_n\}$ such that $y_n \rightarrow x$. If $A_n = (-\infty, y_n]$ then $A_1
\supseteq A_2 \supseteq \ldots$ and $\cap_{n \ge 1}A_n = A$; $A$ is a subset of
all $A_n$. Therefore,
\[
\lim_{n \rightarrow \infty}P(A_n) = P(A)
\]
that is, 
\[
\lim_{y_n \rightarrow x^+}F(y_n) = F(x),
\]
so that $F$ is right continuous.
\end{proof}
\begin{rem}
The same reasoning does not work from the left side. Suppose the sequence $\{y_n\}$
is monotonically increasing with limit $x$ and we consider the sets $A_n = (-\infty,
y_n]$. Then the union of these sets is $(-\infty, x)$ and not $(-\infty, x]$.
\end{rem}

\begin{defn}\label{c2d7}
A random variable whose range is a countable set is called a discrete random 
variable. If $X$ is a discrete random variable, its probability mass function,
also called pmf, is $f_X(x) = P(X = x)$.
\end{defn}

In terms of $f$, $F(x) = \sum_{x^\op \le x}P(x^\op)$.

\begin{defn}\label{c2d8}
A random variable whose range is uncountable is called a continuous random
variable. If $X$ is a continuous random variable, its density function $f$ is
defined as
\[
\int_a^b f(x)dx = P(a \le X \le b),
\]
where $F$ is its cdf.
\end{defn}

In terms of $f$,
\[
F(x) = \int_{-\infty}^x f(x)dx.
\]

\begin{prop}\label{c2p2}
Let $F$ be the cdf of a random variable $X$. Then:
\begin{enumerate}
\item $P(X=x) = F(x) - F(x^-)$, where $F(x^-) = \lim_{y \uparrow x}F(y)$.
\item $P(x < X \le y) = F(y) - F(x)$.
\item $P(X > x) = 1 - F(x)$.
\item If $X$ is continuous then $F(b) - F(a) = P(a < x < b) = P(a \le x < b) =
P(a < x \le b) = P(a \le x \le b)$.
\end{enumerate}
\end{prop}
\begin{proof}
Let $\{y_n\}$ be a monotone increasing sequence of numbers with limit $x$. Then
$P(X \le y_n) = P(A_n)$, where $A_n = (-\infty, y_n]$. The sets $\{A_n\}$ are 
also monotone increasing with union $A = (-\infty, x)$. Therefore,
\[
\lim_{n \rightarrow \infty}P(X \le y_n) = P(A).
\]
Since $(-\infty, x] = (-\infty, x) + \{x\}$, $P(X=x) = F(x) - F(x^-)$.

The set $A = \{x < X \le y\}$ can be written as $(-\infty, x] \cup A = 
(-\infty, y]$. Therefore, $P(A) = F(y) - F(x)$.

Since the set $X > x$ is a complement of the set $X \ge x$ or equivalently,
$x \le X$, $P(X > x) = 1 - P(x \le X) = 1 - F(x)$.

If $F$ is continuous then $F(x^-) = F(x) = F(x^+)$ so that $P(X=x) = 0$. Since
$(a, b) \cup {b} = (a, b]$, $P(X \in (a, b)) + P(X = b) = P(X \in (a, b])$ so 
that $P(X \in (a, b)) = P(X \in (a, b])$. Other relations can be similarly 
proved.
\end{proof}

\begin{defn}\label{c2d9}
Let $X$ be a random variable with cdf $F$. Then its quantile function $Q$ is 
defined as $Q(p) = \inf\{x \;|\; F(x) \ge p\}$, for $p \in [0, 1]$.
\end{defn}
\begin{rem}
The right continuity of $F$ makes $\inf\{x \;|\; F(x) \ge p\} = 
\inf\{x \;|\; F(x) > p\}$.
\end{rem}

\begin{defn}\label{c2d10}
Random variables $X$ and $Y$ are said to be equal in distribution if $F_X(x) = 
F_Y(x)$ for all $x \in \sor$. Equality in distribution is written as $X
\stackrel{d}{=} Y$.
\end{defn}
We will illustrate this tricky concept with an example. Consider a game between
two players which involves tossing a coin. Player `A' wins £1 if a coin lands
head and loses £1 if it lands tail. His opponent `B' loses £1 if the coin lands
head and wins £1 otherwise. The sample space is $\{H, T\}$. Let $X$ and $Y$ be 
the pay-off functions of the two players. Then $X(H) = 1, X(T) = -1, Y(H) = -1$
and $Y(T) = 1$. If the coin is fair, $F_X(-1) = 1/2, F_X(1) = 1, F_Y(-1) = 1/2,
F_Y(1) = 1/2$ although $Y = -X$. If the coin was unfair then we would not have
had this situation. For in that case, $F_X(-1) = 1 - p, F_X(1) = p, F_Y(-1) = p,
F_Y(1) = 1 - p$.

Examples of random variables.
\begin{enumerate}
\item $X \sim \delta_a$ if $P(X=a) = 1$ and $P(x) = 0$ for $x \ne 1$. Its cdf is
\begin{equation}\label{c2e1}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
1 & \text{ if } x \ge a.
\end{cases}
\end{equation}
This  reminds of `integral of Dirac delta function' being the `Heaviside step
function'.

\item Let $k > 1$ be an integer and let 
\begin{equation}\label{c2e2}
P(X=x) = f(x) = \begin{cases}
\frac{1}{k} & \text{ if } x = 1, 2, \ldots, k \\
0 & \text{ otherwise.}
\end{cases}
\end{equation}
the $X$ is called a uniform discrete distribution. All points $1, 2, \ldots, k$
have a uniform probability of $1/k$. Its cdf is
\begin{equation}
F(x) = \begin{cases}
0 \text{ if } x \le 0 \\
\frac{x}{k} \text{ if } x = 1, 2, \ldots, k \\
1 \text{ if } x > k.
\end{cases}
\end{equation}

\item $X \sim \dBer(p)$ if $X(0) = p, X(1) = 1 - p$. Its cdf if
\begin{equation}\label{c2e3}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
p & \text{ if } x \in [0, 1) \\
1 & \text{ if } x \ge 1.
\end{cases}
\end{equation}

\item $X \sim \dBin(n, p)$ is $X$ is the number of times a coin lands heads if
it is tossed $n$ times and if the probability of landing head is $p$. Its pmf
is 
\begin{equation}\label{c2e4}
P(X=x) = \binom{n}{x}p^x(1 - p)^{n-x}.
\end{equation}
Its cdf is
\begin{equation}\label{c2e5}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
 & \text{ if } \sum_{k=0}^x\binom{n}{k}p^k(1 - p)^{n-k} \\
 & 1 \text{ if } x \ge n.
\end{cases}
\end{equation}

\item The random variable that counts the number of coin tosses before the first
head is called the geometric random variable. If the coin has a probability $p$
of landing head then $Y \sim \dGeo(p)$ and
\begin{equation}\label{c2e6}
f(X=x) = P(X=x) = (1 - p)^{x-1}p.
\end{equation}
Its cdf is
\begin{eqnarray}
F(x) &=& \sum_{k=1}^x (1-p)^{k-1}p \nonumber \\
 &=& p\sum_{k=0}^{x-1}(1 - p)^k \nonumber \\
 &=& p\frac{(1 - (1 - p)^x)}{1 - 1 + p} \nonumber \\
 &=& (1 - (1 - p)^x) \label{c2e7}
\end{eqnarray}

\item A Poisson random variable with parameter $\lambda$ is defined as 
\begin{equation}\label{c2e8}
f(x) = e^{-\lambda}\frac{\lambda^x}{x!}.
\end{equation}

\item We can also have a uniform, continuous random variable. Its density is
\begin{equation}\label{c2e9}
f(x) = \begin{cases}
=\frac{1}{b - a} & \text{ if } if x \in [a, b] \\
= 0 & \text{otherwise}.
\end{cases}
\end{equation}
Its cdf is
\begin{equation}\label{c2e10}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
\frac{x - a}{b - a} & \text{ if } x \in [a, b] \\
1 & \text{ if } x > b.
\end{cases}
\end{equation}

\item $X \sim \dNor(\mu, \sigma^2)$ has a normal or gaussian random variable with 
parameters $\mu$ and $\sigma$ if its density is,
\begin{equation}\label{c2e10}
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{\sigma^2}\right).
\end{equation}
Here $x \in \sor$. Its cdf is the error function,
\begin{equation}\label{c2e11}
F(x) = \frac{1}{2}\left(1 + \erf\left(\frac{x - \mu}{\sigma\sqrt{2}}\right)\right),
\end{equation}
where the error function is defined as 
\begin{equation}\label{c2e12}
\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt.
\end{equation}
A standard normal distribution has parameters $\mu = 0, \sigma = 1$. The density 
and cdf of a standard normal distribution are denoted by $\varphi$ and $\Phi$ 
respectively. A standard normal random variable is denoted by $Z$. Properties of
the normal random variable will be proved in later chapters after we introduce the
mean and the variance.

\item $X \sim \dExp(\beta)$ has an exponential density given by
\begin{equation}\label{c2e13}
f(x) = \frac{1}{\beta}\exp\left(-\frac{x}{\beta}\right), 
\end{equation}
if $x > 0$. Its cdf is 
\begin{equation}\label{c2e14}
F(x) = 1 - \exp\left(\frac{x}{\beta}\right).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dGam(\alpha, \beta)$ if
\begin{equation}\label{c2e15}
f(x) = \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha - 1}
\exp\left(-\frac{x}{\beta}\right),
\end{equation}
where
\begin{equation}\label{c2e16}
\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1}e^{-x}dx.
\end{equation}
Clearly,
\begin{equation}\label{c2e17}
\dExp(\beta) = \dGam(1, \beta).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dBet(\alpha, \beta)$ if
\begin{equation}\label{c2e18}
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha, \beta)}
       x^{\alpha - 1}(1 - x)^{\beta - 1}
\end{equation}

\item $X \sim t(\nu)$ if
\begin{equation}\label{c2e19}
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	   \frac{1}{\left(1 + \frac{x^2}{\nu}\right)^{(\nu+1)/2}},
\end{equation}
for $x \in \sor$. As $\Gamma(1/2) = \sqrt{\pi}$, the density of $X \sim t(1)$ is
\begin{equation}\label{c2e20}
f(x) = \frac{1}{\pi}\frac{1}{1 + x^2}.
\end{equation}
This is called the Cauchy density. In proposition \ref{c2p5} we show that as 
$\nu \infty$ the $t$ distribution becomes the standard normal distribution.

\item $X \sim \chi^2(\nu)$ if $x > 0$ and
\begin{equation}\label{c2e21}
f(x) = \frac{1}{\Gamma(\nu/2) 2^{\nu/2}}x^{\nu/2 - 1}e^{-x/2}.
\end{equation}
\end{enumerate}

We prove a few propositions about the random variables introduced so far.
\begin{prop}\label{c2p3}
Let $X_1 \sim \dBin(n_1, p), X_2 \sim \dBin(n_2, p)$ then $X_1 + X_1 \sim
\dBin(n_1 + n_2, p)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. Then $Y = y, X_1 = x_1, X_2 = x_2$ iff $y = x_1 + x_2$ and
hence $P(Y = y) = P(X_1 = x_1, X_2 = y - x_1) = P(X_1 = x_1)P(X_2 = y - x_1)$
as $X_1$ and $X_2$ are independent. Thus,
\begin{eqnarray*}
f_Y(y) &=& \sum_{k=0}^y\binom{n_1}{k}p^{k}(1 - p)^{n_1 - k}\binom{n_2}{y - k}
p^{y - k}(1 - p)^{n_2 - y + k} \\
 &=& p^y(1 - p)^{n_1 + n_2 - y}
 \sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k}
\end{eqnarray*}
The number of ways to choose $y$ objects from a pool of $n_1 + n_2$ objects is
to choose $k$ objects from among $n_1$ and $y - k$ from among $n_2$ for each
$k = 0, \ldots y$. Therefore, by this combinatorial argument,
\[
\sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k} = \binom{n_1+n_2}{y},
\]
so that
\[
f_Y(y) = \binom{n_1+n_2}{y}p^y(1 - p)^{n_1 + n_2 - y}.
\]
Thus, $Y \sim \dBin(n_1 + n_2, p)$.
\end{proof}

\begin{prop}\label{c2p4}
If $X_1 \sim \dPoi(\lambda_1)$ and $X_2 \sim \dPoi(\lambda_2)$ then $X_1 +
X_2 \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. If $Y = x, X_1 = x_1$ then $X_2 = x - x_1$. Thus,
\begin{eqnarray*}
f_Y(x) = P(Y = x) &=& \sum_{k=0}^xP(X_1 = k, X_2 = y - k) \\
 &=& \sum_{k=0}^xP(X_1=k)P(X_2=y-k) \\
 &=& e^{-\lambda_1}\frac{\lambda_1^{k}}{k!}e^{-\lambda_2}\sum_{k=0}^x
 \frac{\lambda_2^{x - k}}{(x - k)!} \\
 &=& e^{-\lambda_1 - \lambda_2}\sum_{k=0}^x
 \frac{\lambda_1^{k}\lambda_2^{x-k}}{k!(x-k)!} \\
 &=& \frac{e^{-\lambda_1 - \lambda_2}}{x!}\sum_{k=0}^x
 \frac{x!}{k!(x-k)!}\lambda_1^{k}\lambda_2^{x-k} \\
 &=& e^{-(\lambda_1 + \lambda_2)}\frac{(\lambda_1 + \lambda_2)^x}{x!}.
\end{eqnarray*} 
Thus, $Y \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{proof}

\begin{prop}\label{c2p4}
If $X \sim t(\nu)$ then as $\nu \rightarrow \infty$, $X \sim \dNor(0, 1)$.
\end{prop}
\begin{proof}
We will use Striling approximation for $\Gamma$ function,
\begin{equation}\label{c2e22}
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
			\left(1 + O\left(\frac{1}{z}\right)\right)
\end{equation}
For large $z$, it becomes
\[
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
\]
The $t$ density is
\[
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	\left(1 + \frac{x^2}{\nu}\right)^{-\nu/2}\left(1 + \frac{x^2}{\nu}\right)^{-1/2}
\]
so that
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{1/2} &=& 1 \\
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{\nu/2} &=& e^{-x^2/2}
\end{eqnarray*}
and
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\frac{1}{\sqrt{\pi\nu}}
\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)} &=& \frac{1}{\sqrt{2e}}
\sqrt{\frac{\nu}{\nu+1}}\left(1 + \frac{1}{\nu}\right)^{\nu/2}\sqrt{\nu+1} \\
 &=& \frac{\sqrt{\nu}}{\sqrt{2e}}\sqrt{e} \\
 &=& \frac{1}{\sqrt{2}}
\end{eqnarray*}
so that
\[
\lim_{\nu\rightarrow\infty}f(x) = \frac{1}{\sqrt{2}}e^{-x^2},
\]
which is the standard normal density.
\end{proof}

\end{document}
