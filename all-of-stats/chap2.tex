\documentclass{article}
\include{common}
\begin{document}
\begin{defn}\label{c2d1}
Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra on 
$\Omega$.  Then members of $\mathcal{F}$ are called measurable sets.
\end{defn}

If a set $A$ is in $\sigma$-algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ then it
is also in the $\sigma$-algebra $\mathcal{F}_1 \cap \mathcal{F}_2$. The smallest
$\sigma$-algebra containing $A$ is called the $\sigma$-algebra generated by $A$
and is denoted by $\sigma(A)$. An example of such a $\sigma$-algebra is the one
generated by open intervals of $\sor$. It is called the Borel $\sigma$-algebra.

\begin{defn}\label{c2d2}
Let $(\Omega, \mathcal{F})$ be a measurable space. A function $m:\Omega
\rightarrow [0, \infty]$ is called a measure if
\begin{itemize}
\item $m(\varnothing) = 0$,
\item If $A_1, A_2, \ldots$ are pair-wise disjoint members of $\mathcal{F}$ then
\[
m\left(\bigcup_{k \ge 1}A_k\right) = \sum_{k \ge 1}m(A_k).
\]
\end{itemize}
\end{defn}
If the range of $m$ is $[0, 1]$ then it is called the probability measure and is
denoted by $P$. The triple $(\Omega, \mathcal{F}, m)$ is called a measure space
while $(\Omega, \mathcal{F}, m)$ is called a probability space. For a probability
space, $P(\Omega) = 1$. Since  $A \cup A^c = \Omega$ is a disjoing union of 
members of $\mathcal{F}$, it follows that $P(A) + P(A^c) = P(\Omega) = 1$.

\begin{defn}\label{c2d3}
Let $(X, \Sigma_X)$ and $(Y, \Sigma_Y)$ be measurable spaces. A map $f: X 
\rightarrow Y$ is said to be measurable if $f^{-1}(B) \in \Sigma_X$ for all
$B \in \Sigma_Y$.
\end{defn}

\begin{defn}\label{c2d4}
Let $(\Omega, \mathcal{F}, P)$ be a probability space then a random variable
is a measurable map $X: \Omega \rightarrow \sor$, where it is understood that
the $\sigma$-algebra on $\sor$ is the Borel $\sigma$-algebra. In other words,
if for every open interval $(a, b)$, $P^{-1}((a, b)) \in \mathcal{F}$.
\end{defn}

Examples of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ be the sample space of a coin-toss. Then $X(H)=1,
X(T)=0$ is a random variable.
\item If $\Omega = \{H, T\}$ and a person wins $20$ quid if the coin lands head
or loses $10$ if the coin lands tail then $X(H) = 20, X(T) = -10$ is also a 
random variable. Thus, a random variable associates a real number to every 
outcome of an experiment.
\item If $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ then $X(\omega) = x, Y(
\omega) = y, R(\omega) = \sqrt{x^2 + y^2}, L(\omega) = |x + y|$ are some examples
of random variables.
\end{enumerate}

\begin{defn}\label{c2d5}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X$ be a random variable
on it. If $A$ is a measurable subset of $\sor$ then $P(A) = P(X^{-1}(A))$.
\end{defn}

Thus, $P$ is also a probability measure on $(\sor, \mathcal{B}, m)$ where 
$\mathcal{B}$ is the Borel $\sigma$-algebra on $\sor$ and $m$ is a Lebesgue 
measure. (That is, if $I = (a, b)$ then $m(I) = b - a$.)

Examples of probability of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ such that $P(H) = 0.3, P(T) = 0.7$. If $X(H)=1,
X(T)=0$ then $P(1) = 0.3, P(0) = 0.7$.
\item If $\Omega = \{HH, HT, TH, TT\}$ be the sample space of two tosses of a
coin for which the probability of getting a head is $p$. If $P(HH) = 2, P(HT) = 
P(TH) = 1, P(TT) = 0$ then $P(2) = p^2, P(1) = 2p(1-p),P(0) = (1 - p)^2$.
\item Let $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ denote a dart-board whose
payoff is denoted by 
\[
X(\omega) = \begin{cases}
0 \text{ if } \sqrt{x^2 + y^2} > 1/2 \\
1 \text{ if } \sqrt{x^2 + y^2} \le 1/2,
\end{cases}
\]
where $\omega$ is the point where the dart lands and its coordinates are 
$(x, y)$, then $P(X = 0) = 1/2$ and $P(X = 1) = 1/2$.
\end{enumerate}

\begin{defn}\label{c2d6}
Let $X$ be a random variable on a probability space $(\Omega, \mathcal{F}, P)$.
Its cumulative distribution function $F_X: \sor \rightarrow [0, 1]$ is defined
as $F_X(x) = P(X \le x)$.
\end{defn}

Unless otherwise stated, we will always assume that all random variables are 
defined over a probability space $(\Omega, \mathcal{F}, P)$.
\begin{prop}\label{c2p1}
Let $X$ be a random variable with cfd $F$. If $I$ is the interval $(a, b)$ then
$P(I) = F(b) - F(a)$.
\end{prop}
\begin{proof}
$F(b) = P(X \le b) = P(\omega \in X^{-1}((-\infty, b]))$. Since $(-\infty, a]
\cup (a, b] = (-\infty, b]$ and the union is of disjoint sets,
\[
P((-\infty, A]) + P((a, b]) = P((-\infty, b]) \Rightarrow F(a) + P((a, b]) 
= F(b).
\]
Now $P(I) = P((a, b)) = P((a, b])$ because $(a, b] = (a, b) \cup \{b\}$ and 
$P(\{b\}) = 0$. Therefore, $P(I) = F(b) - F(a)$.
\end{proof}

\begin{thm}\label{c2t1}
If $X$ and $Y$ are random variables with cdfs $F$ and $G$ then $F(x) = G(x)
\Rightarrow P(X \in A) = P(Y \in A)$ for all measurable sets $A$. 
\end{thm}
\begin{proof}
Here $A$ is a measurable subset of $\mathcal{B}$, the Borel $\sigma$-algebra on
$\sor$. It can be written as a countable union of open intervals or their 
complements. Any countable union of sets can be written as a countable union of
disjoint sets. Therefore, any $A \in \mathcal{B}$ can be written as a countable
union of pair-wise disjoint open intervals.

Since $F(x) = G(x)$, by proposition \ref{c2p1}, $P(X \in A) = P(Y \in A)$ for all
open intervals $A$. Since any measurable set can be written as a countable union
of open intervals or their complements and for each of these sets, $A$, 
$P(X \in A) = P(Y \in A)$, the equality is also true for all measurable sets.
\end{proof}

\begin{thm}\label{c2t2}
Let $F$ be the cdf of a random variable $X$ then:
\begin{enumerate}
\item $x_1 < x_2 \Rightarrow F(x_1) \le F(x_2)$,
\item $F(x) \rightarrow 0$ as $x \rightarrow -\infty$ and $F(x) \rightarrow 1$
as $x \rightarrow \infty$ and
\item $F$ is right continuous for all $x \in \sor$.
\end{enumerate}
\end{thm}
\begin{proof}
$F(x_2) = P(X \le x_2) = P(\omega \in (-\infty, x_2]) = P(\omega \in (-\infty, x_1]
\cup (x_1, x_2]) = P(X \le x_1) + P((x_1, x_2]) \ge F(x_1)$.

As $x \rightarrow \infty$, the set $X \le x$ becomes all of $\sor$ so that 
$X^{-1}(\sor)$ is $\Omega$ and hence $P(X^{-1}(\sor)) = P(\Omega) = 1$. Likewise,
as $x \rightarrow -\infty$, the set $X \le x$ becomes $\varnothing$ for which
the probability is zero.

To prove right continuity at $x$, consider a monotonically decreasing sequence 
$\{y_n\}$ such that $y_n \rightarrow x$. If $A_n = (-\infty, y_n]$ then $A_1
\supseteq A_2 \supseteq \ldots$ and $\cap_{n \ge 1}A_n = A$; $A$ is a subset of
all $A_n$. Therefore,
\[
\lim_{n \rightarrow \infty}P(A_n) = P(A)
\]
that is, 
\[
\lim_{y_n \rightarrow x^+}F(y_n) = F(x),
\]
so that $F$ is right continuous.
\end{proof}
\begin{rem}
The same reasoning does not work from the left side. Suppose the sequence $\{y_n\}$
is monotonically increasing with limit $x$ and we consider the sets $A_n = (-\infty,
y_n]$. Then the union of these sets is $(-\infty, x)$ and not $(-\infty, x]$.
\end{rem}

\begin{defn}\label{c2d7}
A random variable whose range is a countable set is called a discrete random 
variable. If $X$ is a discrete random variable, its probability mass function,
also called pmf, is $f_X(x) = P(X = x)$.
\end{defn}

In terms of $f$, $F(x) = \sum_{x^\op \le x}P(x^\op)$.

\begin{defn}\label{c2d8}
A random variable whose range is uncountable is called a continuous random
variable. If $X$ is a continuous random variable, its density function $f$ is
defined as
\[
\int_a^b f(x)dx = P(a \le X \le b),
\]
where $F$ is its cdf.
\end{defn}

In terms of $f$,
\[
F(x) = \int_{-\infty}^x f(x)dx.
\]

\begin{prop}\label{c2p2}
Let $F$ be the cdf of a random variable $X$. Then:
\begin{enumerate}
\item $P(X=x) = F(x) - F(x^-)$, where $F(x^-) = \lim_{y \uparrow x}F(y)$.
\item $P(x < X \le y) = F(y) - F(x)$.
\item $P(X > x) = 1 - F(x)$.
\item If $X$ is continuous then $F(b) - F(a) = P(a < x < b) = P(a \le x < b) =
P(a < x \le b) = P(a \le x \le b)$.
\end{enumerate}
\end{prop}
\begin{proof}
Let $\{y_n\}$ be a monotone increasing sequence of numbers with limit $x$. Then
$P(X \le y_n) = P(A_n)$, where $A_n = (-\infty, y_n]$. The sets $\{A_n\}$ are 
also monotone increasing with union $A = (-\infty, x)$. Therefore,
\[
\lim_{n \rightarrow \infty}P(X \le y_n) = P(A).
\]
Since $(-\infty, x] = (-\infty, x) + \{x\}$, $P(X=x) = F(x) - F(x^-)$.

The set $A = \{x < X \le y\}$ can be written as $(-\infty, x] \cup A = 
(-\infty, y]$. Therefore, $P(A) = F(y) - F(x)$.

Since the set $X > x$ is a complement of the set $X \ge x$ or equivalently,
$x \le X$, $P(X > x) = 1 - P(x \le X) = 1 - F(x)$.

If $F$ is continuous then $F(x^-) = F(x) = F(x^+)$ so that $P(X=x) = 0$. Since
$(a, b) \cup {b} = (a, b]$, $P(X \in (a, b)) + P(X = b) = P(X \in (a, b])$ so 
that $P(X \in (a, b)) = P(X \in (a, b])$. Other relations can be similarly 
proved.
\end{proof}

\begin{defn}\label{c2d9}
Let $X$ be a random variable with cdf $F$. Then its quantile function $Q$ is 
defined as $Q(p) = \inf\{x \;|\; F(x) \ge p\}$, for $p \in [0, 1]$.
\end{defn}
\begin{rem}
The right continuity of $F$ makes $\inf\{x \;|\; F(x) \ge p\} = 
\inf\{x \;|\; F(x) > p\}$.
\end{rem}

\begin{defn}\label{c2d10}
Random variables $X$ and $Y$ are said to be equal in distribution if $F_X(x) = 
F_Y(x)$ for all $x \in \sor$. Equality in distribution is written as $X
\stackrel{d}{=} Y$.
\end{defn}
We will illustrate this tricky concept with an example. Consider a game between
two players which involves tossing a coin. Player `A' wins £1 if a coin lands
head and loses £1 if it lands tail. His opponent `B' loses £1 if the coin lands
head and wins £1 otherwise. The sample space is $\{H, T\}$. Let $X$ and $Y$ be 
the pay-off functions of the two players. Then $X(H) = 1, X(T) = -1, Y(H) = -1$
and $Y(T) = 1$. If the coin is fair, $F_X(-1) = 1/2, F_X(1) = 1, F_Y(-1) = 1/2,
F_Y(1) = 1/2$ although $Y = -X$. If the coin was unfair then we would not have
had this situation. For in that case, $F_X(-1) = 1 - p, F_X(1) = p, F_Y(-1) = p,
F_Y(1) = 1 - p$.

Examples of random variables.
\begin{enumerate}
\item $X \sim \delta_a$ if $P(X=a) = 1$ and $P(x) = 0$ for $x \ne 1$. Its cdf is
\begin{equation}\label{c2e1}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
1 & \text{ if } x \ge a.
\end{cases}
\end{equation}
This  reminds of `integral of Dirac delta function' being the `Heaviside step
function'.

\item Let $k > 1$ be an integer and let 
\begin{equation}\label{c2e2}
P(X=x) = f(x) = \begin{cases}
\frac{1}{k} & \text{ if } x = 1, 2, \ldots, k \\
0 & \text{ otherwise.}
\end{cases}
\end{equation}
the $X$ is called a uniform discrete distribution. All points $1, 2, \ldots, k$
have a uniform probability of $1/k$. Its cdf is
\begin{equation}
F(x) = \begin{cases}
0 \text{ if } x \le 0 \\
\frac{x}{k} \text{ if } x = 1, 2, \ldots, k \\
1 \text{ if } x > k.
\end{cases}
\end{equation}

\item $X \sim \dBer(p)$ if $X(0) = p, X(1) = 1 - p$. Its cdf if
\begin{equation}\label{c2e3}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
p & \text{ if } x \in [0, 1) \\
1 & \text{ if } x \ge 1.
\end{cases}
\end{equation}

\item $X \sim \dBin(n, p)$ is $X$ is the number of times a coin lands heads if
it is tossed $n$ times and if the probability of landing head is $p$. Its pmf
is 
\begin{equation}\label{c2e4}
P(X=x) = \binom{n}{x}p^x(1 - p)^{n-x}.
\end{equation}
Its cdf is
\begin{equation}\label{c2e5}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
 & \text{ if } \sum_{k=0}^x\binom{n}{k}p^k(1 - p)^{n-k} \\
 & 1 \text{ if } x \ge n.
\end{cases}
\end{equation}

\item The random variable that counts the number of coin tosses before the first
head is called the geometric random variable. If the coin has a probability $p$
of landing head then $Y \sim \dGeo(p)$ and
\begin{equation}\label{c2e6}
f(X=x) = P(X=x) = (1 - p)^{x-1}p.
\end{equation}
Its cdf is
\begin{eqnarray}
F(x) &=& \sum_{k=1}^x (1-p)^{k-1}p \nonumber \\
 &=& p\sum_{k=0}^{x-1}(1 - p)^k \nonumber \\
 &=& p\frac{(1 - (1 - p)^x)}{1 - 1 + p} \nonumber \\
 &=& (1 - (1 - p)^x) \label{c2e7}
\end{eqnarray}

\item A Poisson random variable with parameter $\lambda$ is defined as 
\begin{equation}\label{c2e8}
f(x) = e^{-\lambda}\frac{\lambda^x}{x!}.
\end{equation}

\item We can also have a uniform, continuous random variable. Its density is
\begin{equation}\label{c2e9}
f(x) = \begin{cases}
=\frac{1}{b - a} & \text{ if } if x \in [a, b] \\
= 0 & \text{otherwise}.
\end{cases}
\end{equation}
Its cdf is
\begin{equation}\label{c2e10}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
\frac{x - a}{b - a} & \text{ if } x \in [a, b] \\
1 & \text{ if } x > b.
\end{cases}
\end{equation}

\item $X \sim \dNor(\mu, \sigma^2)$ has a normal or gaussian random variable with 
parameters $\mu$ and $\sigma$ if its density is,
\begin{equation}\label{c2e10}
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{\sigma^2}\right).
\end{equation}
Here $x \in \sor$. Its cdf is the error function,
\begin{equation}\label{c2e11}
F(x) = \frac{1}{2}\left(1 + \erf\left(\frac{x - \mu}{\sigma\sqrt{2}}\right)\right),
\end{equation}
where the error function is defined as 
\begin{equation}\label{c2e12}
\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt.
\end{equation}
A standard normal distribution has parameters $\mu = 0, \sigma = 1$. The density 
and cdf of a standard normal distribution are denoted by $\varphi$ and $\Phi$ 
respectively. A standard normal random variable is denoted by $Z$. Properties of
the normal random variable will be proved in later chapters after we introduce the
mean and the variance.

\item $X \sim \dExp(\beta)$ has an exponential density given by
\begin{equation}\label{c2e13}
f(x) = \frac{1}{\beta}\exp\left(-\frac{x}{\beta}\right), 
\end{equation}
if $x > 0$. Its cdf is 
\begin{equation}\label{c2e14}
F(x) = 1 - \exp\left(\frac{x}{\beta}\right).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dGam(\alpha, \beta)$ if
\begin{equation}\label{c2e15}
f(x) = \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha - 1}
\exp\left(-\frac{x}{\beta}\right),
\end{equation}
where
\begin{equation}\label{c2e16}
\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1}e^{-x}dx.
\end{equation}
Clearly,
\begin{equation}\label{c2e17}
\dExp(\beta) = \dGam(1, \beta).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dBet(\alpha, \beta)$ if
\begin{equation}\label{c2e18}
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha, \beta)}
       x^{\alpha - 1}(1 - x)^{\beta - 1}
\end{equation}

\item $X \sim t(\nu)$ if
\begin{equation}\label{c2e19}
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	   \frac{1}{\left(1 + \frac{x^2}{\nu}\right)^{(\nu+1)/2}},
\end{equation}
for $x \in \sor$. As $\Gamma(1/2) = \sqrt{\pi}$, the density of $X \sim t(1)$ is
\begin{equation}\label{c2e20}
f(x) = \frac{1}{\pi}\frac{1}{1 + x^2}.
\end{equation}
This is called the Cauchy density. In proposition \ref{c2p5} we show that as 
$\nu \infty$ the $t$ distribution becomes the standard normal distribution.

\item $X \sim \chi^2(\nu)$ if $x > 0$ and
\begin{equation}\label{c2e21}
f(x) = \frac{1}{\Gamma(\nu/2) 2^{\nu/2}}x^{\nu/2 - 1}e^{-x/2}.
\end{equation}
\end{enumerate}

We prove a few propositions about the random variables introduced so far.
\begin{prop}\label{c2p3}
Let $X_1 \sim \dBin(n_1, p), X_2 \sim \dBin(n_2, p)$ then $X_1 + X_1 \sim
\dBin(n_1 + n_2, p)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. Then $Y = y, X_1 = x_1, X_2 = x_2$ iff $y = x_1 + x_2$ and
hence $P(Y = y) = P(X_1 = x_1, X_2 = y - x_1) = P(X_1 = x_1)P(X_2 = y - x_1)$
as $X_1$ and $X_2$ are independent. Thus,
\begin{eqnarray*}
f_Y(y) &=& \sum_{k=0}^y\binom{n_1}{k}p^{k}(1 - p)^{n_1 - k}\binom{n_2}{y - k}
p^{y - k}(1 - p)^{n_2 - y + k} \\
 &=& p^y(1 - p)^{n_1 + n_2 - y}
 \sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k}
\end{eqnarray*}
The number of ways to choose $y$ objects from a pool of $n_1 + n_2$ objects is
to choose $k$ objects from among $n_1$ and $y - k$ from among $n_2$ for each
$k = 0, \ldots y$. Therefore, by this combinatorial argument,
\[
\sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k} = \binom{n_1+n_2}{y},
\]
so that
\[
f_Y(y) = \binom{n_1+n_2}{y}p^y(1 - p)^{n_1 + n_2 - y}.
\]
Thus, $Y \sim \dBin(n_1 + n_2, p)$.
\end{proof}

\begin{prop}\label{c2p4}
If $X_1 \sim \dPoi(\lambda_1)$ and $X_2 \sim \dPoi(\lambda_2)$ then $X_1 +
X_2 \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. If $Y = x, X_1 = x_1$ then $X_2 = x - x_1$. Thus,
\begin{eqnarray*}
f_Y(x) = P(Y = x) &=& \sum_{k=0}^xP(X_1 = k, X_2 = y - k) \\
 &=& \sum_{k=0}^xP(X_1=k)P(X_2=y-k) \\
 &=& e^{-\lambda_1}\frac{\lambda_1^{k}}{k!}e^{-\lambda_2}\sum_{k=0}^x
 \frac{\lambda_2^{x - k}}{(x - k)!} \\
 &=& e^{-\lambda_1 - \lambda_2}\sum_{k=0}^x
 \frac{\lambda_1^{k}\lambda_2^{x-k}}{k!(x-k)!} \\
 &=& \frac{e^{-\lambda_1 - \lambda_2}}{x!}\sum_{k=0}^x
 \frac{x!}{k!(x-k)!}\lambda_1^{k}\lambda_2^{x-k} \\
 &=& e^{-(\lambda_1 + \lambda_2)}\frac{(\lambda_1 + \lambda_2)^x}{x!}.
\end{eqnarray*} 
Thus, $Y \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{proof}

\begin{prop}\label{c2p4}
If $X \sim t(\nu)$ then as $\nu \rightarrow \infty$, $X \sim \dNor(0, 1)$.
\end{prop}
\begin{proof}
We will use Striling approximation for $\Gamma$ function,
\begin{equation}\label{c2e22}
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
			\left(1 + O\left(\frac{1}{z}\right)\right)
\end{equation}
For large $z$, it becomes
\[
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
\]
The $t$ density is
\[
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	\left(1 + \frac{x^2}{\nu}\right)^{-\nu/2}\left(1 + \frac{x^2}{\nu}\right)^{-1/2}
\]
so that
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{1/2} &=& 1 \\
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{\nu/2} &=& e^{-x^2/2}
\end{eqnarray*}
and
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\frac{1}{\sqrt{\pi\nu}}
\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)} &=& \frac{1}{\sqrt{2e}}
\sqrt{\frac{\nu}{\nu+1}}\left(1 + \frac{1}{\nu}\right)^{\nu/2}\sqrt{\nu+1} \\
 &=& \frac{\sqrt{\nu}}{\sqrt{2e}}\sqrt{e} \\
 &=& \frac{1}{\sqrt{2}}
\end{eqnarray*}
so that
\[
\lim_{\nu\rightarrow\infty}f(x) = \frac{1}{\sqrt{2}}e^{-x^2},
\]
which is the standard normal density.
\end{proof}

A few more propositions about univariate distributions can be proved easily after
introducing moments and generating functions.

\begin{defn}\label{c2d11}
Let $X$ and $Y$ be discrete random variables. The joint pmf of $X$ and $Y$ 
measures the probability of the event $X = x$ and $Y = y$. Thus,
$f(x, y) = P(X = x, Y = y)$.
\end{defn}

\begin{defn}\label{c2d12}
Let $X$ and $Y$ be continuous random variables. The joint density function of
$X$ and $Y$, $f: \sor^2 \rightarrow [0, \infty)$ his a function for which
\[
\iint_{\sor^2} f(x, y)dxdy = 1
\]
and if If $A \subset \sor^2$ then
\[
P((x, y) \in A) = \iint_A f(x, y)dxdy.
\]
\end{defn}

In either case, the cdf is defined as $F_{X, Y}(x, y) = P(X \le x, Y \le y)$.

\begin{defn}\label{c2d13}
If $f(x, y)$ is the joint probability function of discrete random variables then
the marginal pmf of $X$ is
\[
f_X(x) = \sum_y f(x, y)
\]
and the marginal pmf of $Y$ is
\[
f_Y(y) = \sum_x f(x, y).
\]
\end{defn}

\begin{defn}\label{c2d14}
If $f(x, y)$ is the joint probability function of continuous random variables then
the marginal density function of $X$ is
\[
f_X(x) = \int_{-\infty}^\infty f(x, y) dy
\]
and the marginal pmf of $Y$ is
\[
f_Y(y) = \int_{-\infty}^\infty f(x, y) dx.
\]
\end{defn}

\begin{defn}\label{c2d15}
If $X$ and $Y$ are discrete random variables with joint pmf $f(x, y)$ then the
conditional pmf of $X$ given $Y = y$ is
\[
f_{X|Y=y} = \frac{f(x, y)}{f_Y(y)},
\]
provided $f_Y(y) \ne 0$. The conditional pmf of $Y$ given $X = x$ is defined 
similarly.
\end{defn}

\begin{defn}\label{c2d16}
If $X$ and $Y$ are continuous random variables with joint density $f(x, y)$ then
the conditional density of $X$ given $Y = y_0$ is
\[
f_{X|Y = y_0}(x) = \frac{f(x, y_0)}{f_Y(y_0)},
\]
provided $f_Y(y) \ne 0$ and
\[
P(X \in A|Y = y_0) = \int_A f_{X|Y=y_0}dx.
\]
\end{defn}
Note that for a particular value of $y = y_0$, $f_{X|y=y_0}$ is a function of $x$
alone.

\begin{defn}\label{c2d17}
If $X$ and $Y$ are discrete random variables with joint pmf $f(x, y)$ then they
are independent if for all $A, B \in \mathcal{F}$, $P(X \in A, Y \in B) = 
P(X \in A)P(X \in B)$. Likewise, if $X$ and $Y$ are continuous random variables 
with joint density $f(x, y)$ then they are independent if $f(x, y) = f_X(x)f_Y(y)$ 
for all $x, y$.
\end{defn}

\begin{prop}\label{c2p6}
If $X$ and $Y$ are discrete random variables then they are independent iff
$f(x, y) = f_X(x)f_Y(y)$ for all $x, y$.
\end{prop}
\begin{proof}
Let $A = \{x\}$ and $B = \{y\}$, where $x, y$ are arbitrary values of $X$ and
$Y$. Then by definition \ref{c2d17}, $P(X=x, Y=y) = P(X=x)P(Y=y) \Rightarrow
f(x, y) = f_X(x)f_Y(y)$.

Since $X$ and $Y$ are discrete random variables, the sets $A$ and $B$ of their
values are countable. Therefore,
\begin{eqnarray*}
P(X \in A, Y \in B) &=& \sum_{x \in A, y \in B}P(X = x, Y = y) \\
 &=& \sum_{x \in A, y \in B}f(x, y) \\
 &=& \sum_{x\in A, y \in B}f_X(x)f_Y(y) \\
 &=& \sum_{x \in A}f_X(x)\sum_{y \in B}f_Y(y) \\
 &=& P(X \in A)P(Y \in B).
\end{eqnarray*}
\end{proof}

The definitions of joint, marginal and conditional distributions for a pair of
random variables can be generalised to a finite number $n$ of random variables.
In particular,
\begin{defn}\label{c2d18}
If $X_1, \ldots, X_n$ are independent, that is the joint pmf or density $f$ 
factorises as
\[
f_{\vec{X}}(x_1, \ldots, x_n) = f(x_1)\cdots f(x_n)
\]
then we say that $X_1, \ldots, X_n$ are independent and identically distributed
, also called iid, and write $X_1, \ldots, X_n \sim f$ or $X_1, \ldots, X_n \sim
F$, if $F$ is the cdf of $f$. We also call $x_1, \ldots, x_n$ to be a random 
sample of size $n$ from $f$ (or $F$).
\end{defn}

\begin{defn}\label{c2d19}
A vector $\vec{X} = (X_1, \ldots, X_n)$, where $X_i$ are non-negative integers,
is said to have a multinomial distribution with parameters $n$ and $\vec{p}$,
denoted as $\vec{X} \sim \dMul(n, \vec{p})$, if $p_1 + \cdots + p_n = 1$ and 
\begin{equation}\label{c2e25}
f(\vec{x}) = \binom{n}{x_1 \ldots x_n}p_1^{x_1}\cdots p_n^{x_n}.
\end{equation}
\end{defn}

\begin{defn}\label{c2d20}
A vector $\vec{X}$ has a multivariate normal distribution, denoted as $\vec{X}
\sim \dNor(\vec{\mu}, \Sigma)$ if it has a density
\begin{equation}\label{c2e26}
f(\vec{x}; \vec{\mu}, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}
\exp\left(-\frac{1}{2}(\vec{x} - \vec{\mu})^T\Sigma(\vec{x} - \vec{\mu})\right).
\end{equation}
The positive-definite matrix $\Sigma$ is called the covariance matrix and 
$|\Sigma|$ is its determinant.
\end{defn}

\section{Solutions to problems}
\begin{enumerate}
\item[1.] $F(x) = P(X \le x)$ so that $F(x^\pm) = P(X \le x^\pm)$. Now we have
the set relation $\{X \le x^-\} \cup \{X = x\} = \{x \le x^+\}$ and the union
is of two disjoint sets. Therefore, $P(\{X \le x^-\})+P(X = x)=P(\{X \le x^+\})$
so that $P(X=x) = F(x^+) - F(x^-)$.

\item[2.] We first check that $P(2) + P(3) + P(5) = 1$ so that we have a valid
pmf. The cdf is $F(x) = 0$ for $x < 1$, $F(2) = 0.1, F(3) = 0.2, F(5) = 1$ and 
$F(x) = 1$ for $x > 1$. Figure \ref{c2f1} shows the plot.
\begin{figure}
\includegraphics[scale=0.6]{c2p2}
\caption{Problem 2}\label{c2f1}
\end{figure}

\item[3.] Refer to proposition \ref{c2p2}.

\item[4.] Give that
\[
f(x) = \begin{cases}
0.25 & \text{ if } x \in (0, 1) \\
0.375 & \text{ if } x \in (3, 5) \\
0 & \text{ otherwise.}
\end{cases}
\]
Clearly, 
\[
\int_{-\infty}^\infty f(x)dx = 0.25 \times (1 - 0) + 0.375 \times (5 - 3) = 1
\]
so that $f$ is a valid density. The cdf $F$ is calculated as follows: 
\[
F(x) = \begin{cases}
0 & \text{ if } x \le 0 \\
\frac{x}{4} & \text{ if } x \in (0, 1) \\
\frac{1}{4} & \text{ if } x \in [1, 3] \\
\frac{1}{4} + \frac{3}{8}x & \text{ if } x \in (3, 5) \\
1 & \text{ if } x \ge 5
\end{cases}
\]

Let $Y = 1/X$. $0 < y \le 1/5 \Rightarrow x \ge 5$ so that
\[
F_Y(1/5) = \int_5^\infty f(x) dx = 0.
\]
$1/5 < y < 1/3 \Rightarrow x \in (3, 5)$ so that
\[
F_Y(1/3) = F_Y(1/5) + \int_3^5 f(x)dx = \int_3^5\frac{3}{8}dx = \frac{3}{4}.
\]
$1/3 \le y \le 1 \Rightarrow x \in [1, 3]$ so that
\[
F_Y(1) = F_Y(1/3) + \int_1^3 f(x)dx = \frac{3}{4} + 0 = \frac{3}{4}.
\]
$y > 1 \Rightarrow x \in (1, 1/y)$ so that
\[
F_Y(y) = F_Y(1) + \int_{1/y}^1 f(x)dx 
= \frac{3}{4} + \frac{1}{4}\left(1 - \frac{1}{y}\right).
\]
Thus,
\[
F_Y(y) = \begin{cases}
0 & \text{ if } y <= 1/5 \\
\frac{3}{4} & \text{ if } y \in (1/5, 1/3) \\
\frac{3}{4} & \text{ if } y \in [1/3, 1] \\
\frac{3}{4} + \frac{1}{4}\left(1 - \frac{1}{y}\right) & \text{ if } y > 1
\end{cases}
\]

\item[5.]Refer to proposition \ref{c2p6}.

\item[6.]$Y$ is a Bernoulli random variable. $Y(x) = 1$ iff $x \in A$, $Y(x) = 0$
otherwise. Now,
\[
P(Y = 1) = \int_A f_X(x)dx = f_Y(1)
\]
and
\[
P(Y = 0) = \int_{A^c}f(x) dx = f_Y(0).
\]
Clearly, $P(Y=0) + P(Y=1) = 1$ so that 
\[
F_Y(y) = \begin{cases}
0 & \text{ if } y < 0 \\
\int_A f_X(x)dx & \text{ if } y \in [0, 1) \\
1 & \text{ if } y \ge 1.
\end{cases}
\]

\item[7.] Given that $X \sim \dUni(0, 1)$ and $Y \sim \dUni(0, 1)$ are independent.
Let $Z = \min(X, Y)$. Therefore, $P(Z > z) = P(\min(X, Y) > z) = P(X > z
\cup Y > z) = P(X > z) + P(Y > z) - P(X > z \cap Y > z) = P(X > z) + 
P(Y > z) - P(X > z)P(Y > z)$, as $X$ and $Y$ are independent. Now, $P(X > z) =
1 - P(X \le z) = 1 - F_X(z)$ and similarly for $P(Y > z)$. Therefore, $P(Z > z)
= (1 - F_X(z)) + (1 - F_Y(z)) - (1 - F_X(z))(1 - F_Y(z))$. The cdf for uniform
distribution is $F_X(z) = z$ so that
\[
P(Z > z) = 2(1 - z) - (1 - z)^2
\]
so that $F_Z(z) = P(Z \le z) = 1 - P(Z > z) = 1 - 2(1 - z) + (1 - z)^2 = z^2$
so that $f_Z(z) = 2z$ is the required density.

\item[8.] If $X^+ = \max(X, 0)$ then $X^+$ will never have negative values. 
Therefore, if $G$ is its cdf then $G(y) = 0$ if $y < 0$. For $X \ge 0$, $X^+=X$
so that $P(G \le y)$ for $y > 0$ is $P(X \le y) = F_X(y)$. Thus,
\[
G(y) = \begin{cases}
0 & \text{ if } y < 0 \\
F_X(y) & \text{ if } y \ge 0.
\end{cases}
\]

\item[9.] From equation \eqref{c2e16},
\[
F(x) = 1 - \exp\left(\frac{x}{\beta}\right).
\]
If $F(x) = q$ then $e^{x/\beta} = 1 - q$ so that $x = \beta\ln(1 - q)$, that is
\begin{equation}\label{c2e27}
F^{-1}(q) = \beta\ln(1 - q).
\end{equation}

\item[10.] Let $U = g(X), V = h(Y)$ be random variables which are functions of 
$X$ and $Y$. Therefore, $g^{-1}(a)$ is a subset of $\sor$ whose values are image
of a subset of the sample space under $X$. Similarly for $h^{-1}(b)$. Since $X$
and $Y$ are independent, $P(X \in g^{-1}(a), Y \in h^{-1}(b)) = P(X \in g^{-1}(a))
P(Y \in h^{-1}(b)) \Rightarrow P(g(X) = a, h(Y) = b) = P(g(X)=a)P(h(Y)=b) 
\Rightarrow P(U=a, V=b) = P(U=a)P(V=b)$. This proof is applicable to discrete, 
$X, Y$.

If $X$ and $Y$ are continuous, 
\[
f(x=g^{-1}(a), y=g^{-1}(b)) = f_X(x=g^{-1}(a))f_Y(y=h^{-1}(b)) 
\]
so that $f(g(x)=a, h(y)=b) = f_X(g(x)=a)f_Y(h(y)=b) 
\Rightarrow f^\ast(u=a, v=b) = f_X^\ast(u=a)f_Y^\ast(v=b)$, where $f_X^\ast(u)
= f_X(g(x))$ and analogously for other functions.

\item[11.] Suppose we toss the coin $n$ times. If $h, t \le n$ are the number of 
heads and tails, then $P(h = x, t = y)$ will be zero whenever $x + y \ne n$, yet
$P(h = x)$ and $P(t = y)$ are not zero. Therefore, the two random variables are 
not independent.

Let $H$ be the random variable measuring the number of heads, $T$ measuring tails
and $N$ the number of tosses. Then $P(H = h, T = t, N = n) = P(H=h, T=t|N=n)
P(N=n) = P(H=h, T=n-h)P(N=n)$. Now,
\[
P(H=h, T=n-h) = \binom{n}{h}p^{h}(1 - p)^{n-h}
\]
and
\[
P(N=n) = e^{-\lambda}\frac{\lambda^n}{n!}
\]
so that
\begin{eqnarray*}
P(H = h, T = t, N = n) &=& 
	\frac{n!}{h!(n-h)!}p^h(1-p)^{n-h}e^{-\lambda}\frac{\lambda^n}{n!} \\
 &=& \frac{p^h}{h!}\frac{(1-p)^{n-h}}{(n-h)!} e^{-\lambda}\lambda^h \lambda^{n-h} \\
 &=& \frac{(p\lambda)^h}{h!}\frac{((1-p)\lambda)^{n-h}}{(n-h)!}e^{-(p + (1 - p))\lambda} \\
 &=& e^{-p\lambda}\frac{(p\lambda)^h}{h!} e^{-q\lambda}\frac{(q\lambda)^{n-h}}{(n-h)!},
\end{eqnarray*}
where we used $q = 1 - p$ for notational simplicity. The right hand side is a 
product of two Poisson random variables with parameters $p\lambda$ and $q\lambda$
and values $h$ and $n - h$. Thus, $P(H=h, T=t, N=n)$ factorises as a product of
two random variables whose sum is always $n$.
\end{enumerate}
\end{document}
